---
title: "Region1CyanoData2014"
author: "Bryan Milstead"
date: "Monday, December 29, 2014"
output: html_document
---
<!---
use these command instead of the knit icon if you want the data and work loaded into the R workspace
  library(knitr)
      a<-getwd()
        if(substr(a,nchar(a)-8,nchar(a))=='Reg1Cyano')  {setwd('./Data2014/')
          } else {if(substr(a,nchar(a)-7,nchar(a))!='Reg1Cyano') print('WARNING: Wrong Working Directory')}
  knit('Data2014.rmd')
-->
To Do List
-------------------------
* Need data from RI, and ME
* Need answers to questions for CT & UNH
* Map locations and send to contacts for verifications
* Add WBIDs


Introduction
-------------------------
During the summer of 2014 the New England states initiated a monitoring program for cyanobacteria in lakes.  Participants included state and tribal governments, local watershed associations, EPA Region 1, and the EPA Atlantic Ecology Division. 

The task now is to collate all the data into a standardized format.  Below are the data processing steps used and some questions to be resolved for each data contributor.

A simple excel template for data entry is available here: https://github.com/willbmisled/Reg1Cyano/blob/master/Data2014/Region1CyanobacteriaDataEntryTemplate.xls?raw=true

This document is available here: https://github.com/willbmisled/Reg1Cyano/blob/master/Data2014/Data2014.md

The details of all data processing steps including notes and rcode are available here: https://github.com/willbmisled/Reg1Cyano/blob/master/Data2014/Data2014.Rmd

The collated dataset is available here in .csv format (hint: right click on the link and choose “Save As” to save the file to your computer, otherwise, it will open in your browser): https://github.com/willbmisled/Reg1Cyano/blob/master/Data2014/Data2014.csv?raw=true

Data Definitions
-------------------------

Field  | Units | Description
------------- | ------------- | -------------
**Order:**|(integer)|Leave Blank or number observations consecutively
**YourName:**|(text)|Enter the name of the person entering the data
**Organization:**|(text)|Enter the name of the organization responsible for collecting the samples-e.g. Vermont DEC
**SiteID:**|(text)|If the location has a site ID from your organization enter it here
**WaterbodyName:**|(text)|Enter the name of the lake: be consistent with spelling and capitalization
**State:**|(lookup)|Choose the two letter state code from the drop down list
**SiteLocation:**|(text)|Some organizations have site names within a lake.  Enter these here.
**SampleYear:**|(YYYY)|Year sample was collected in format YYYY (e.g., 2014)
**SampleMonth:**|(MM)|Month sample was collected in integer format (e.g., months 1 to 12 )
**SampleDay:**|(DD)|Day sample was collected in integer format (e.g., day 1 to 31 )
**NameOfSamplers:**|(text)|Add the names of the field crew separated by commas.
**WeatherConditions:**|(text)|General Weather conditions separated by commas.  E.g., Cloudy, Windy, Cold
**SampleLocation:**|(lookup)|Where (in the lake) was the sample collected followed by the replicate number? WithinLake=WL1, WL2, or WL3; ShoreSide=SS1, SS2, SS3; Can also add Calibration or Blank for validation readings or Other.
**SampleMethod:**|(lookup)|How was the sample collected? Grab = Grab sample for surface blooms; Composite =Composite; Integrated = Integrated tube sample; Validation = Use this for Blanks and Calibration Standards; Other = give details in the comments section.
**Depth:**|(integer)|Within Lake samples are at 3m and Shoreside are at 1m.  If samples taken at different depths not the depth here and enter details in the comments section.  Leave blank for standards and blanks.
**Latitude:**|(decimal)|Latitude in Decimal Degrees with 4 decimal places or degrees minutes seconds
**Longitude:**|(decimal)|Longitude in Decimal Degrees with 4 decimal places or degrees minutes seconds
**SampleHour:**|(HH)|Hour sample was taken in 24 hour format
**SampleMinutes:**|(MM)|Minute sample was taken in integer format
**Filtered:**|(TRUE/FALSE)|Was the sample filtered?
**Frozen:**|(TRUE/FALSE)|Was the sample frozen?
**Parameter:**|(lookup)|Phycocyanin or Chlorophyll?
**Value:**|(decimal)|Reading from the fluorometer
**Units:**|(lookup)|What were the units recorded.  If not "RFU", "µ/L", or mg/l flag the entry and add comment with the units used. 
**Rep:**|(integer)|If you made more than one measurement per sample or took more than one sample per site assign a replicate number to each observations and add notes in the comment field.
**Fluorometer:**|(lookup)|Type of fluorometer used?
**AnalysisYear:**|(YYYY)|Year sample was analyzed in format YYYY (e.g., 2014)
**AnalysisMonth:**|(MM)|Month sample was analyzed in integer format (e.g., months 1 to 12 )
**AnalysisDay:**|(DD)|Day sample was analyzed in integer format (e.g., day 1 to 31 )
**AnalysisHour:**|(HH)|Hour sample was analyzed in 24 hour format
**AnalysisMinutes:**|(MM)|Minute sample was analyzed in integer format
**GPSType:**|(text)|How was the location determined.  GPS (type), map, or google?
**Photos:**|(TRUE/FALSE)|Where photos taken?
**Flag:**|(TRUE/FALSE)|Add a flag for any data line that needs further validation or processing
**Comments:**|(text)|Add details of flags or any notes about the data line or sample.

Data Sources
-------------------------
Information on how the data were provided by the States and Organizations that participated in the monitoring.


### Vermont

* Data received from Hilary Snook (forwarded from Angela Shambaugh-VTDEC) on 11/13/14
* Original file name: VERMONT Beagle FluoroQuick data_Summer 2014.xlsx renamed VTDEC.xlsx
* Responses to my queries answered by Angela Shambaugh on 12/19/2014

**Questions for Angela with responses from Angela (AS), Hilary Snoork (HS) and me (BM):**

* Do we need to treat the two groups of data ("calibrated pigment readings" & "after calibration loss") separately?
    - AS: Depends on Hilary’s experience when re-calibrating my Beagle unit.  I don’t know if he was able to tell how much drift there was over the summer.
    - HS: I’ll hopefully be checking these out today. 
    - BM: tables combined and the "after calibration loss" lines flagged.
* What are the differences between the two groups of data?
    - AS: I apparently invalidated the instrument calibration halfway through the summer by mistakenly starting the full calibration process and then incorrectly exiting.  The check standards Hilary provided were not at all consistent with expected reading so I stopped reporting phycocyanin units and switched to relative fluorescence units.  
* What is the difference between "blank" and "blank(2)" in the "station" field?
    - AS: Blank refers to a DI sample read at the beginning of each session, blank 2 refers to a second reading of the same sample at the end of the session.
    - BM: leave as is for now.
* For parameter what do: "channel 1 hi", "channel 1 lo", and channel 2 lo" represent?
    - AS: If I understood the Beagle unit correctly, Channel 1 refers to the phycocyanin filter and channel 2 to the chlorophyll filter.  It wasn’t clear to me the difference between high and low – Hilary may be able to answer that better.  Hopefully after recalibrating the instrument, we can interpolate and change all the relative fluorescent units to phycocyanin.  Currently, any number associated with a channel will be RFU.  Anything associated with the terms ‘phycocyanin’ or ‘chlorophyll’ will be biomass (as µg/L).
    - HS: Hi/Lo is automatically selected by the instrument based on sample fluorescence and does not need to be manually selected by the user. This was pretty cryptic in the user manual and I will clarify this at the upcoming meeting.
    - BM: add units="RFU"  for all samples where parameter= channel 1 or 2; add units="µg/L" for all samples where parameter="phycocyanin or chlorophyll"; change paramater="channel 1 (hi or lo)" to "Phycocyanin"; change paramater="channel 2 (hi or lo)" to "Chlorophyll"
* Where were the samples taken?  Shoreside? or Within Lake?  I'm assuming that the 1m depth samples are Shoreside and the 3m or greater depth samples are within lake but would like to verify.
    - AS: All locations from the Champlain TMDL sites (#4 – 51) will be offshore.  RT 78 Access is shoreline as is the Black Bridge location. The others ( Highgate Cliffs, Highgate Springs, Phillipsburg and QE BM-b) are all off shore.  As part of our cyanobacteria monitoring, we take only surface samples for microcystin analysis – hence any surface scums at offshore locations will be surface grabs. 
    - BM: change "RT 78 Access" and "Black Bridge" to SS1; the rest are WL1
* What are the units for fluorometer readings?
    - AS: Beagle units are µg/L, I think.  Hilary can confirm.
    - HS: Yes, readout is in micrograms/liter (ppb)
    - BM: µg/L
* For the "sample state" VTDEC reports the conditions as  c("filtered, CDOM, fresh", "fresh","frozen","DI", "24 hours old", "white", "orange", "blue","", "red", "green", "yellow","pink").  What information is important?  For each condition we need to know whether the sample was frozen (TRUE/FALSE) or filtered (TRUE/FALSE) before the fluoremeter reading was taken. 
    - AS: The colors under ‘sample state’ refer to the solid check standards provided by Hilary so the terms frozen/filtered do not apply.  The 24hr sample old sample was not frozen or filtered.  All CDOM samples were filtered and not frozen.  All fresh samples were unfrozen and unfiltered.  If it is easier, I can re-send my data using the EXCEL spreadsheet you sent out.
    - BM: all samples except those for sample.state=='Frozen' changed to Frozen=FALSE; samples for sample.state="...CDOM..." changed to Filtered==TRUE, the rest are Filtered=FALSE.
* Please check the file VTDEC_Locations.csv.  I have some of the locations but not all.  The locations I have were gleaned from the lake Champlain reports.  In some cases I used Google Earth to get the locations from the descriptions so I don't have a lot of confidence in the locations.  Can you verify the locations (I can send a map of the locations if you wish) and help me with the ones that are missing?
    - AS: I added lat lons to your table for the missing locations.  If you took the lat lon for the other stations from the Champlain TMDL files, they should be correct.  The updates are attached.

**Data steps**-View code in Data2014.rmd document to see details.

* table="calibrated pigment readings" in VTDEC.xlsx saved as VTDEC_calibrated.csv; read into R
* table="after calibration loss" in VTDEC.xlsx saved as VTDEC_uncalibrated.csv; read into R;Flag=TRUE;Comment="Uncertain Calibration Status"
* both tables combined into df=VT
* Build VTDEC data.frame
* Add to data.frame Data2014

```{r VTDEC, include=FALSE, echo=FALSE, cache=FALSE} 
#read data and combine tables
  VT<-read.csv("VTDEC_calibrated.csv") #read the calbibrated data
    VT$Flag<-FALSE #add Flag field
    VT$Comments<-VT$notes  #rename notes to comments
    nrow(VT) #444
  
  VT1<-read.csv("VTDEC_uncalibrated.csv") #read the uncalbrated data
    VT1$Flag<-TRUE #add Flag field
    VT1$Comments<-paste("Uncertain Calibration Status;", VT1$notes)  #add comment and notes to comments
    nrow(VT1) #392
  
  VT<-rbind(VT,VT1) #combine the datasets
    nrow(VT) #836

#Build VTDEC
  #Order=sequential ordering of the raw data
    VTDEC<-data.frame(Order=1:nrow(VT))
  #Add name and org
    VTDEC$YourName<-"Angela Shambaugh"  
    VTDEC$Organization<-"VTDEC"
  #Add SiteID from VT$station
    VTDEC$SiteID<-VT$station
    #fix spellings
      as.data.frame(table(VTDEC$SiteID,useNA='ifany'))
      VTDEC$SiteID[VTDEC$SiteID=="QE BM - b"]<-"QE BM b"
      VTDEC$SiteID[VTDEC$SiteID=="highgate Springs, VT"]<-"Highgate Springs"
    #remove blanks and standards 
      VTDEC$SiteID[grep('blank',VTDEC$SiteID)]<-NA
      VTDEC$SiteID[grep('standard',VTDEC$SiteID)]<-NA
  #Add WaterbodyName
    VTDEC$WaterbodyName<-NA
  #Add State	
    VTDEC$State<-"VT"
  #Add SiteLocation	
    VTDEC$SiteLocation<-NA
  # Add SampleYear	SampleMonth	SampleDay	
    a<-matrix(as.numeric(unlist(strsplit(VT$Sample.Date,'/'))),nrow=nrow(VT),ncol=3,byrow=TRUE)
      VTDEC$SampleYear<-a[,3] 
      VTDEC$SampleMonth<-a[,1] 
      VTDEC$SampleDay<-a[,2] 
  #Add NameOfSamplers	
    VTDEC$NameOfSamplers<-NA
  #Add WeatherConditions	
    VTDEC$WeatherConditions<-NA
  #Add SampleLocation	
    VTDEC$SampleLocation<-"WL1"
    VTDEC$SampleLocation[grep('Black',VT$station)]<-'SS1'
    VTDEC$SampleLocation[grep('Access',VT$station)]<-'SS1'
    VTDEC$SampleLocation[grep('blank',VT$station)]<-'Blank'
    VTDEC$SampleLocation[grep('standard',VT$station)]<-'Calibration'
    table(VTDEC$SampleLocation,useNA='ifany')
  #Add SampleMethod from VT$sample.type & VT&collection.type; blanks and standards from VT$station
    table(VT$sample.type,useNA='ifany') #654 whole water samples and 182 blanks and standards
    table(VT$collection.type,useNA='ifany') #grab=104;hose=550; blanks & standards=182
    VTDEC$SampleMethod<-VT$collection.type
      #rename methods to match template
        VTDEC$SampleMethod[VTDEC$SampleMethod=="grab"]<-'Grab'
        VTDEC$SampleMethod[VTDEC$SampleMethod=="hose"]<-'Integrated'
      #add blanks and standards
        VTDEC$SampleMethod[grep('blank',VT$station)]<-VT$station[grep('blank',VT$station)]
        VTDEC$SampleMethod[grep('standard',VT$station)]<-VT$station[grep('standard',VT$station)]
        table(VTDEC$SampleMethod,useNA='ifany')
   #Add Depth  
    VTDEC$Depth<-VT$depth..m.
  #Add Latitude	Longitude
    loc<-read.csv("VTDEC_Locations.csv") #read the locations
    a<-merge(VTDEC,loc,by='SiteID',all.x=T) #merge locations and siteIDs
    a<-a[order(a$Order),] #reorder
    VTDEC$Latitude<-a$Latitude
    VTDEC$Longitude<-a$Longitude
  #Add SampleHour	SampleMinutes	
    VTDEC$SampleHour<-NA
    VTDEC$SampleMinutes<-NA
  #Add Filtered	
    VTDEC$Filtered<-FALSE
    VTDEC$Filtered[grep('filtered',VT$sample.state)]<-TRUE
    table(VTDEC$Filtered)
  #Add Frozen
    VTDEC$Frozen<-FALSE
    VTDEC$Frozen[grep('frozen',VT$sample.state)]<-TRUE
    table(VTDEC$Frozen)
  #Add Parameter & Value
    VTDEC$Parameter<-VT$parameter
    VTDEC$Value<-VT$Reading
  #Add Units and update parameter
    VTDEC$Units<-paste('\u03BC','g/l',sep='')
    #change channel 1 to Parameter=Phycocyanin and Units=RFU
      a<-grep('1',VTDEC$Parameter)
        VTDEC$Parameter[a]<-'Phycocyanin'
        VTDEC$Units[a]<-'RFU'
    #change channel 2 to Parameter=Chlorophyll and Units=RFU
      a<-grep('2',VTDEC$Parameter)
        VTDEC$Parameter[a]<-'Chlorophyll'
        VTDEC$Units[a]<-'RFU'
    #change spelling
      VTDEC$Parameter[VTDEC$Parameter=='phycocyanin']<-'Phycocyanin'
      VTDEC$Parameter[VTDEC$Parameter=='chlorophyll']<-'Chlorophyll'
  #Add Rep
    VTDEC$Rep<-VT$rep
  #Add Fluorometer
    VTDEC$Fluorometer<-'Beagle'
  #Add AnalysisYear  AnalysisMonth	AnalysisDay	AnalysisHour	AnalysisMinutes
    VT[VT$Analysis.date=='','Analysis.date']<-"NA/NA/NA" #replace missing values
    a<-matrix(as.numeric(unlist(strsplit(VT$Analysis.date,'/'))),nrow=nrow(VT),ncol=3,byrow=TRUE)
      VTDEC$AnalysisYear<-a[,3] 
      VTDEC$AnalysisMonth<-a[,1] 
      VTDEC$AnalysisDay<-a[,2] 
    VTDEC$AnalysisHour<-NA
    VTDEC$AnalysisMinutes<-NA
  #Add GPSType	Photos	Flag	Comments
    VTDEC$GPSType<-NA
    VTDEC$Photos<-FALSE
    VTDEC$Flag<-VT$Flag
    VTDEC$Comments<-VT$Comments
#add to Data2014
  Data2014<-VTDEC #this creates Data2014
```

### Charles River Watershed Association

* Data received from Elisabeth Cianciola on 12/12/14
* Original file name: CRWA_Region1CyanobacteriaDataEntryTemplate.xls renamed CRWA.xls
* Resaved as CRWA_RAW.csv for import into R

**Questions for Elisabeth with her responses**

* Were all samples collected with the integrated tube sampler?
    - EC: Yes
* What were the units for the fluorometer readings?  RFU? µg/L?
    - EC: µg/L
    
**Data steps**-View code in Data2014.rmd document to see details.

* Data originally entered in a version of Region1CyanobacteriaDataEntryTemplate.xls so little massaging was needed.
* CWRA_RAW.csv imported into R
* Each field verified with consistency checks
* Field updated where necessary.
* Add to data.frame Data2014

```{r CRWA, include=FALSE, echo=FALSE, cache=FALSE} 
#read data 
  CRWA<-read.csv("CRWA_RAW.csv") #read the data
  names(CRWA)[c(15,16,23,24)]<-c("Filtered1","Frozen1","Photos1","Flag1") #rename fields that ended in "?"
#check fields for consistency
  table(CRWA$YourName,useNA='ifany')
  table(CRWA$Organization,useNA='ifany')
  table(CRWA$SiteID,useNA='ifany')
  table(CRWA$WaterbodyName,useNA='ifany')
  table(CRWA$State,useNA='ifany')
  table(CRWA$SiteLocation,useNA='ifany')
  table(CRWA$NameOfSamplers,useNA='ifany')
  table(CRWA$WeatherConditions,useNA='ifany')
  table(CRWA$Depth,useNA='ifany')
  table(CRWA$Latitude,useNA='ifany')
  table(CRWA$Longitude,useNA='ifany')
  table(CRWA$Parameter,useNA='ifany')
  summary(CRWA$Value)
  table(is.na(CRWA$Value))
  table(CRWA$Fluorometer,useNA='ifany')
  table(CRWA$GPSType,useNA='ifany')
  table(CRWA$Comments,useNA='ifany')
#update or modify fields
  #Add Order=sequential ordering of the raw data
    CRWA$Order<-1:nrow(CRWA)
  # Add SampleYear	SampleMonth	SampleDay	Drop SampleDate
    a<-matrix(as.numeric(unlist(strsplit(CRWA$SampleDate,'/'))),nrow=nrow(CRWA),ncol=3,byrow=TRUE)
      CRWA$SampleYear<-a[,3] 
      CRWA$SampleMonth<-a[,1] 
      CRWA$SampleDay<-a[,2]
      CRWA<-CRWA[,-which(names(CRWA)=="SampleDate")]
  # Add AnalysisYear  AnalysisMonth	AnalysisDay	Drop AnalysisDate
    a<-matrix(as.numeric(unlist(strsplit(CRWA$AnalysisDate,'/'))),nrow=nrow(CRWA),ncol=3,byrow=TRUE)
      CRWA$AnalysisYear<-a[,3] 
      CRWA$AnalysisMonth<-a[,1] 
      CRWA$AnalysisDay<-a[,2]
      CRWA<-CRWA[,-which(names(CRWA)=="AnalysisDate")]
  #Add SampleLocation	& SampleMethod
    CRWA$SampleLocation<-"SS1" #all sites at SS1
    CRWA<-CRWA[,-which(names(CRWA)=="TypeOfSample")] #delete earlier name for this field
    CRWA$SampleMethod<-'Integrated' #all collected with tube
  #Add SampleHour	SampleMinutes	
    CRWA$SampleTime[CRWA$SampleTime==""]<-"NA:NA" #replace missing values
    a<-matrix(as.numeric(unlist(strsplit(CRWA$SampleTime,':'))),nrow=nrow(CRWA),ncol=2,byrow=TRUE) #parse
    CRWA$SampleHour<-a[,1]
    CRWA$SampleMinutes<-a[,2]
    CRWA<-CRWA[,-which(names(CRWA)=="SampleTime")] #delete time field
  #Add AnalysisHour  AnalysisMinutes	
    CRWA$AnalysisTime[CRWA$AnalysisTime==""]<-"NA:NA" #replace missing values
    a<-matrix(as.numeric(unlist(strsplit(CRWA$AnalysisTime,':'))),nrow=nrow(CRWA),ncol=2,byrow=TRUE) #parse
    CRWA$AnalysisHour<-a[,1]
    CRWA$AnalysisMinutes<-a[,2]
    CRWA<-CRWA[,-which(names(CRWA)=="AnalysisTime")] #delete time field
  #Add Units and update parameter
    CRWA$Units<-paste('\u03BC','g/l',sep='')
  #Add Rep
    CRWA$Rep<-1
  #Change yes/no field to True/False: Frozen Filtered  Photos	Flag
    #Frozen
      table(CRWA$Frozen1,useNA='ifany')
      CRWA$Frozen<-ifelse(CRWA$Frozen1=="Yes",TRUE,FALSE)
      table(CRWA$Frozen,useNA='ifany')
      CRWA<-CRWA[,-which(names(CRWA)=="Frozen1")] #delete old field
    #Filtered
      table(CRWA$Filtered1,useNA='ifany')
      CRWA$Filtered<-ifelse(CRWA$Filtered1=="YES",TRUE,FALSE)
      table(CRWA$Filtered,useNA='ifany')
      CRWA<-CRWA[,-which(names(CRWA)=="Filtered1")] #delete old field
    #Photos
      table(CRWA$Photos1,useNA='ifany')
      CRWA$Photos<-ifelse(CRWA$Photos1=="YES",TRUE,FALSE)
      table(CRWA$Photos,useNA='ifany')
      CRWA<-CRWA[,-which(names(CRWA)=="Photos1")] #delete old field
    #Flag
      table(CRWA$Flag1,useNA='ifany')
      CRWA$Flag<-FALSE
      table(CRWA$Flag,useNA='ifany')
      CRWA<-CRWA[,-which(names(CRWA)=="Flag1")] #delete old field
  #reorder fields to match template
    CRWA<-CRWA[,names(VTDEC)]
# add to data.frame Data2014
  Data2014<-rbind(Data2014,CRWA)
```

### New Hampshire

* Data received from Sonya Carlson on 11/19/14
* Original file name: NH-2014-11-19-PhycoData.xlsx renamed NHDES.xlsx

**Questions for Sonya: with her responses**

* The locations are in tab "List_Frame_1" and the Data are in the data tab.  Both sets of data appear to be linked by the first column ("#" or "Sample #").  It appears that a some of the lines are out of order (i.e., 2 & 3 and 83 & 84).  This is not a problem I just adjusted them.  Is this okay?
    - SC: Okay
* In the data tab there are 2 entries for Sample#=42.  The first for DESStationID=BEELTUFD is not in the List_Frame_1 tab but the second (DESStationID=BROOSSD) is.  To correct this I used the location information from Sample#=54 (DESStationID=BEELTUFD).  Is this okay?
    - SC: Okay
* For Sample#=135 the DES Station ID is "TOWCAN-GEN
" in the Data tab and "TOWCND-GEN" in the List_Frame_1 tab.  Which is correct?
    - SC: TOWCND-GEN: this is corrected above
* Could you verify the units for the Beagle readings.  You have mg/l but others report µg/L.
    - SC: Should be µg/L-corrected above.
* Are all Turner readings in RFU and all beagle in µg/L?  
    - SC: Yes
* Were all samples except the Grab samples collected with  an integrated sampler (a hose)?
    - SC: Yes


**Data steps**-View code in Data2014.rmd document to see details.

* The fluorometer data are in NHDES.XLSX tab:Data. This is copied to NHDES_RAW.csv and modified as follows:
  - First column ("#") containing a sequential list of numbers is renamed "Order"
  - There are two observations for Order=42;the first does not match the location information but the second does.
  - First occurence of Order=42 changed to Order=144; it is then cut and pasted to the bottom of the sheet.
  - The location information for the new NHDES_Raw:Order=144 data matches NHDES_Locations:Order=54. See NHDES_Locations.csv below. 
  - Order=135 the DES Station ID doesn't match the Location table.  I checked with Sonya and changed the NHDES_RAW value to 'TOWCND-GEN'.
  - The "adjusted turner" and calculated "ratio" fields were deleted.
  - The value fields for the Fluorometry readings are on three lines.  These were combined into a single field name using the following convention: Pigment_Units_Fluorometer_SampleState_FilterState.  These will be split up and renamed later.
* Location information is in NHDES.XLSX tab:List_Frame1.  This is copied to NHDES_Locations.csv and modified as follows:
    - First column ("#") containing a sequential list of numbers is renamed "Order"
    - Missing location information for NHDES_Raw:Order=144 same as information NHDES_Locations:Order=54.  This line is copied and appended to the bottom and the Locations:Order is Changed to 144
    - Observations for Order=2 and Order=3 are reversed between NHDES_Raw.csv and NHDES_Data.csv.  Locations:Order=2 changed to 3 and Locations:Order=3 changed to 2.
    - Observations for Order=83 and Order=84 are reversed between NHDES_Raw.csv and NHDES_Data.csv.  Locations:Order=83 changed to 84 and Locations:Order=84 changed to 83.
* Add to data.frame Data2014

```{r NHDES, include=FALSE, echo=FALSE, cache=FALSE} 
#read the data
  NH<-read.csv("NHDES_RAW.csv") #read the data
  Loc<-read.csv("NHDES_Locations.csv") #read the data
#compare Order in NH and LOC
  all.equal(NH$Order,Loc$Order) #TRUE
#Build NHDES
  #Build Template first
    NH_T<-data.frame(Order=NH$Order)
    NH_T$YourName<-"Sonya Carlson"
    NH_T$Organization<-'NH_T'
    NH_T$SiteID<-NH$DES.Station.ID
    NH_T$WaterbodyName<-Loc$Waterbody  #this is a number but appears to be consistent names is StatonName field
    NH_T$State<-'NH'
    NH_T$SiteLocation<-Loc$Station.Name
    # Add SampleYear  SampleMonth	SampleDay	
        a<-matrix(as.numeric(unlist(strsplit(NH$Sample.Date,'/'))),nrow=nrow(NH),ncol=3,byrow=TRUE)
          NH_T$SampleYear<-a[,3] 
          NH_T$SampleMonth<-a[,1] 
          NH_T$SampleDay<-a[,2] 
    NH_T$NameOfSamplers<-NA
    NH_T$WeatherConditions<-NH$Weather
    #Add SampleLocation
      table(NH$EPA.Designation,useNA='ifany')
      NH_T$SampleLocation<-NH$EPA.Designation
        NH_T$SampleLocation[NH_T$SampleLocation=='SS#1']<-'SS1'
        NH_T$SampleLocation[NH_T$SampleLocation=='SS#2']<-'SS2'
        NH_T$SampleLocation[NH_T$SampleLocation=='SS#3']<-'SS3'
        NH_T$SampleLocation[NH_T$SampleLocation=='WL#1']<-'WL1'
        NH_T$SampleLocation[NH_T$SampleLocation=='WL#2']<-'WL2'
        NH_T$SampleLocation[NH_T$SampleLocation=='WL#3']<-'WL3'
        NH_T$SampleLocation[NH_T$SampleLocation=='WL#4']<-'WL4'
        NH_T$SampleLocation[NH_T$SampleLocation=='GRAB']<-'Other' #change SampleMethod to 'Grab'
      table(NH_T$SampleLocation,useNA='ifany')
    #Add SampleMethod
      NH_T$SampleMethod<-'Integrated'
        NH_T$SampleMethod[NH_T$SampleLocation=='Other']<-'Grab'
    NH_T$Depth<-NH$Depth
    NH_T$Latitude<-Loc$Latitude.DecDeg
    NH_T$Longitude<-Loc$Longitude.DecDeg
    NH_T$SampleHour<-NA 
    NH_T$SampleMinutes<-NA 
    NH_T$Filtered<-NA 
    NH_T$Frozen<-NA 
    NH_T$Parameter<-NA 
    NH_T$Value<-NA 
    NH_T$Units<-NA 
    NH_T$Rep<-NA
    NH_T$Fluorometer<-NA
    NH_T$AnalysisYear<-NA 
    NH_T$AnalysisMonth<-NA
    NH_T$AnalysisDay<-NA
    NH_T$AnalysisHour<-NA
    NH_T$AnalysisMinutes<-NA
    NH_T$GPSType<-NA
    NH_T$Photos<-FALSE
    NH_T$Flag<-FALSE
    NH_T$Comments<-NA
#Loop to add "Filtered"  "Frozen" "Parameter" "Value" "Units" & "Fluorometer"
  NHDES<-c()  #empty data frame
    for(i in c(7:ncol(NH))){
      a<-unlist(strsplit(names(NH)[i],'_')) #get the column name
      b<-NH_T #open the blank template then fill in the field below
        b$Parameter<-a[1]
        b$Units<-a[2]
        b$Fluorometer<-a[3]
        b$Frozen<-ifelse(a[4]=='Frozen','TRUE','FALSE')
        b$Filtered<-ifelse(a[5]=='Filtered','TRUE','FALSE')
        b$Value<-NH[,i]
      NHDES<-rbind(NHDES,b) #add data to data.frame
    }

# add to data.frame Data2014
  Data2014<-rbind(Data2014,NHDES)
```

### Connecticut

* Data received from Tracy Lizotte on 11/17/14
* Original file name: "Cyanobacteria data Summer 2014.xlsx" saved as CTDEEP.xlsx

Questions for Tracy:

* For the Housatonic River (Kettletown) on 7/7/2014 the fluorometer values are "337.23//254.39", "310.87//363.l59", "24.91//7.50", and "2.50//5.50".  Do these represent multiple observations from the same sample or perhaps readings from 2 different samples? For now I am calling these reps and have separated the values.
* What are the measurement units? RFU, µg/L?
* Where were the samples taken?  Shoreside? or Within Lake?
* What depth were the samples taken at?
* How were the samples taken?  Grab samples? Whole Water Samples? Integrated Tube Samples?
* I have some but not all of the Latitude and Longitudes for the lakes.  Please check the file CTDEEP_Locations.csv and verify the locations (I can send a map of the locations if you wish) and fill in missing values. 
*NEW: were any samples frozen?

**Data steps**-View code in Data2014.rmd document to see details.
* Data organized with a separate lake in each tab.  These are read directly into R and the data from the separate sheets are combined into a single data.frame.
```{r CTDEEP1, include=FALSE, echo=FALSE, cache=FALSE} 
#read the data from the Excel files and rbind all of the separate sheets into a single DF
  library(RODBC)
  conn<-odbcConnectExcel2007("CTDEEP.xlsx") # open a connection to the Excel file
  a<-sqlTables(conn)$TABLE_NAME # get list of all sheets
  a<-a[-12] #remove sheet "Station$" from the list
  
  CT<-c() #blank object to store data
  for(i in c(1:length(a))){
    b<-sqlFetch(conn, a[i]) # read a sheet
    CT<-rbind(CT,b)
  }
  close(conn)
```
* Each lake has a parameter field that includes many parameters besides the fluorometry data but most of these are missing values.
* The dataset includes many parameters but only the Chlorophyll and Phycocanin observations will be kept.
* The Parameters with Chlorophyll and Phycocyanin include information on whether the samples were filtered or not. We'll assume that the samples were not frozen.
* The parameter field was parsed as follows (NOTE: observations with ParameterNew=NA were deleted):
```{r CTDEEP2, include=TRUE, echo=FALSE, cache=FALSE} 
  #Add ParameterNew field with just the Parameter names
    CT$ParameterNew<-NA
    CT$ParameterNew[grep('Phycocyanin',CT$Parameter)]<-'Phycocyanin' 
    CT$ParameterNew[grep('Chl a',CT$Parameter)]<-'Chlorophyll' 
  #add Frozen and Filtered fields based on Parameter names
    a<-c(grep("As is",CT$Parameter), grep("unfiltered",CT$Parameter)) #select parameter for unfiltered samples
    CT$Frozen<-FALSE  #all samples appear to have been processed unfrozen
    CT$Filtered<-FALSE #start with all equal FALSE
      CT$Filtered[-a]<-TRUE #changed to TRUE those that do not have "unfiltered" or "AS is" in the parameter name.
  #Show changes
    unique(CT[order(CT$Parameter),c('Parameter','ParameterNew','Filtered','Frozen')])
  #Keep phycocyanin and Chlorophyll data only
   CT<-CT[!is.na(CT$ParameterNew),] #limit to Phyco and Chl parameters only
```
* The value field contains a mix of numeric values, character values, and missing values.  
* The table below shows the number of occurences of each non-numeric value
```{r CTDEEP3, include=TRUE, echo=FALSE, cache=FALSE, warning=FALSE} 
  CT$ValueNew<-as.numeric(CT$Value)
  as.data.frame(table(CT$Value[is.na(CT$ValueNew)],useNA='ifany'))
```
* The first four character values (two number separated by 2 slashes) appear to be two readings for the same sample.  Split these into two lines.  Assign the first to Rep=1 with Value = the first value.  Assign the second to Rep=2 with Value = the second value. 
* Delete observations with Value="not Collected" or NA
```{r CTDEEP4, include=FALSE, echo=FALSE, cache=FALSE} 
  #add Rep field
    CT$Rep<-1
  #add flag field
    CT$Flag<-FALSE
  #copy lines with two readings;change ValueNew to second reading and the Rep to Rep=2
    a<-CT[grep("//",CT$Value),] #select rows
    a$Rep<-2 #change rep to 2
    a$ValueNew<-c(254.39,363.159,7.50,5.50) #Note: 363.l59 changed to 363.159
    a$Flag<-TRUE  #add flag
    a$Comment<-paste('Value Changed from:',a$Value) #add comment
  #change ValueNew for lines with two readings for value to the first reading
    b<-grep("//",CT$Value)
    CT$Flag[b]<-TRUE
    CT$Comment<-paste('Value Changed from:',CT$Value[b])
    CT$ValueNew[grep("//",CT$Value)]<-c(337.23,310.87,24.91,2.50)
  #Append rep=2 lines to CT
    CT<-rbind(CT,a)
  #Delete rows with ValueNew=NA
    nrow(CT) #612+4=616
    CT<-CT[!is.na(CT$ValueNew),]
    nrow(CT) #256
```
* Data massage complete-build CTDEEP
* Add to data.frame Data2014
```{r CTDEEP, include=FALSE, echo=FALSE, cache=FALSE} 
#rename fields to remove spaces and special characters.
names(CT)<-c("StationID","WaterbodyName","Landmark","Date","lab_accession","Parameter","Value","gtG","ltG","UOM","Lab","Note","ParameterNew","Frozen","Filtered","ValueNew","Rep","Flag","Comment")
#view data
  unique(CT$StationID) #SiteID: 12 station IDs 
  unique(CT$WaterbodyName) #WaterbodyName 12 lake names
  unique(CT$Landmark) #SiteLocation 
  unique(CT$Date) #SampleDates in format YYYY-MM-DD
  unique(CT$lab_accession) #all NA
  table(CT$gtG,useNA='ifany') #this must be the detection limit
  table(CT$ltG,useNA='ifany') #this must be the detection limit
  unique(CT$UOM) #Units: NA
  unique(CT$Lab) # all values="DEEP"
  unique(CT$Note) #"Beagle Flourometer"
#build CTDEEP
  CTDEEP<-data.frame(Order=1:nrow(CT))
  CTDEEP$YourName<-'Tracy Lizotte'
  CTDEEP$Organization<-'CTDEEP'
  CTDEEP$SiteID<-CT$StationID
  CTDEEP$WaterbodyName<-CT$WaterbodyName
  CTDEEP$State<-'CT'
  CTDEEP$SiteLocation<-CT$Landmark
  CTDEEP$SampleYear<-2014 
  CTDEEP$SampleMonth<-as.numeric(substr(CT$Date,6,7)) 
  CTDEEP$SampleDay<-as.numeric(substr(CT$Date,9,10)) 
  CTDEEP$NameOfSamplers<-NA
  CTDEEP$WeatherConditions<-NA
  CTDEEP$SampleLocation<-NA
  CTDEEP$SampleMethod<-NA  #probably "Integrated"
  CTDEEP$Depth<-NA
  CTDEEP$Latitude<-NA
  CTDEEP$Longitude<-NA
  CTDEEP$SampleHour<-NA
  CTDEEP$SampleMinutes<-NA
  CTDEEP$Filtered<-CT$Filtered
  CTDEEP$Frozen<-CT$Frozen
  CTDEEP$Parameter<-CT$ParameterNew
  CTDEEP$Value<-CT$ValueNew
  CTDEEP$Units<-NA
  CTDEEP$Rep<-CT$Rep
  CTDEEP$Fluorometer<-'Beagle'
  CTDEEP$AnalysisYear<-NA
  CTDEEP$AnalysisMonth<-NA
  CTDEEP$AnalysisDay<-NA
  CTDEEP$AnalysisHour<-NA
  CTDEEP$AnalysisMinutes<-NA
  CTDEEP$GPSType<-NA
  CTDEEP$Photos<-FALSE
  CTDEEP$Flag<-CT$Flag
  CTDEEP$Comments<-CT$Comment

# add to data.frame Data2014
  Data2014<-rbind(Data2014,CTDEEP)
```

### University of New Hampshire

* Data received from Amanda Murby on 12/29/14
* Original file name: "Region1CyanobacteriaDataEntryTemplate_NewHampshire-UNH2014.xls" saved as UNH.xls
* Additional data to be sent later

**Questions for Amanda:**

* The "Depth"" field has a mixture of depth ranges and single depth values.  Do these represent different sample methods?  For example are the depth ranges (e.g., 0-4) from the integrated tube samplers?  How were the single depth samples collected? 
* The "TypeOfSample" field has many missing values. What are these samples? and, How were they collected?
* Missing Longitude and Latitude?
* What are the measurement units? RFU? µg/L?
* What do the Flags=Yes represent? Most comments for these lines are blank.

**Data steps**-View code in Data2014.rmd document to see details.

* Data originally entered in a version of Region1CyanobacteriaDataEntryTemplate.xls so little massaging was needed.
* UNH_RAW.csv imported into R
* Each field verified with consistency checks
* Field updated where necessary.
* Add to data.frame Data2014

```{r UNH, include=FALSE, echo=FALSE, cache=FALSE} 
#read data 
  UNH<-read.csv("UNH_RAW.csv") #read the data
  names(UNH)[c(15,16,23,24)]<-c("Filtered","Frozen","Photos","Flag") #rename fields that ended in "?"
#remove empty line.
  UNH[338,] #this line contains only the comment "additional data missing" delete the line
  UNH<-UNH[-338,]
#check fields X,X.1-X.17
    table(UNH$X,useNA='ifany') # Appears to be the 6 missing negative values for a blank reading-delete
    table(is.na(UNH[,paste("X.",c(1:17),sep='')]))  #all NA; delete
    UNH<-UNH[,-c(26:ncol(UNH))]
#Review, add, & modify fields as needed
  UNH$Order<-1:nrow(UNH)
  table(UNH$YourName,useNA='ifany') 
  table(UNH$Organization,useNA='ifany')  #'UNH Center for Freshwater Biology' change to:
    UNH$Organization<-'UNH_CFB'
  table(UNH$SiteID,useNA='ifany')  #all NA
  table(UNH$WaterbodyName,useNA='ifany')
    UNH$WaterbodyName[UNH$WaterbodyName=='']<-NA #replace empty cells with NA
  table(UNH$State,useNA='ifany')  #ME=12 NH=313 NA=12
    UNH$State[UNH$State=='']<-NA #replace empty cells with NA
  table(UNH$SiteLocation,useNA='ifany')
  # Add SampleYear  SampleMonth  SampleDay	Drop SampleDate
    table(UNH$SampleDate,useNA='ifany')
      UNH$SampleDate[UNH$SampleDate=='']<-'NA/NA/NA' #replace empty cells with NA/NA/NA
    a<-matrix(as.numeric(unlist(strsplit(UNH$SampleDate,'/'))),nrow=nrow(UNH),ncol=3,byrow=TRUE)
      UNH$SampleYear<-a[,3] 
      UNH$SampleMonth<-a[,1] 
      UNH$SampleDay<-a[,2]
      UNH<-UNH[,-which(names(UNH)=="SampleDate")]
  table(UNH$NameOfSamplers,useNA='ifany')
    UNH$NameOfSamplers[UNH$NameOfSamplers=='']<-NA #replace empty cells with NA
  table(UNH$WeatherConditions,useNA='ifany')
    UNH$WeatherConditions[UNH$WeatherConditions=='']<-NA #replace empty cells with NA
  #rename TypeOfSample to SampleLocation
    names(UNH)[which(names(UNH)=="TypeOfSample")]<-'SampleLocation'
      table(UNH$SampleLocation,useNA='ifany')
        #modify values for consistency
          UNH$SampleLocation[UNH$SampleLocation=='']<-NA #replace empty cells with NA
          unique(Data2014$SampleLocation)
            unique(UNH$SampleLocation)
              UNH$SampleLocation<-gsub("-","",UNH$SampleLocation)
              UNH$SampleLocation<-gsub("Standard","Calibration",UNH$SampleLocation)
  #add field "SampleMethod"
    UNH$SampleMethod<-NA
#Depth is a mix of character and numeric data  
  as.data.frame(table(UNH$Depth,useNA='ifany'))
  #change ranges to max
    a<-grep("-",UNH$Depth) #select rows with depth ranges
      as.data.frame(table(UNH$Depth[a],useNA='ifany'))  #check
        b<-as.numeric(unlist(strsplit(UNH$Depth[a],'-'))) #use the "-" character to split values
        #b[seq(2,length(b),2)] #choose the second value only
          data.frame(UNH$Depth[a],b[seq(2,length(b),2)]) #check
        #update records and add comment
          UNH$Comments[a]<-paste("Depth Changed From:",UNH$Depth[a])
          UNH$Depth[a]<-b[seq(2,length(b),2)] #update records
  table(UNH$Latitude,useNA='ifany')  #missing 105 observations
  table(UNH$Longitude,useNA='ifany') #missing 105 observations
  #parse SampleTime
    table(UNH$SampleTime,useNA='ifany') #All missing
      UNH$SampleHour<-NA
      UNH$SampleMinutes<-NA
        UNH<-UNH[,-which(names(UNH)=="SampleTime")]
  table(UNH$Filtered,useNA='ifany')
  #change field Filtered from Y/N to TRUE/FALSE
    table(UNH$Filtered,useNA='ifany')
      UNH$Filtered[UNH$Filtered=='']<-NA #replace empty cells with NA
      UNH$Filtered<-gsub("Yes",TRUE,UNH$Filtered)
      UNH$Filtered<-gsub("No",FALSE,UNH$Filtered)
        UNH$Filtered<-as.logical(UNH$Filtered) #convert to logical
  #change field Frozen from Y/N to TRUE/FALSE
    table(UNH$Frozen,useNA='ifany')
      UNH$Frozen[UNH$Frozen=='']<-NA #replace empty cells with NA
      UNH$Frozen<-gsub("Yes",TRUE,UNH$Frozen)
      UNH$Frozen<-gsub("No",FALSE,UNH$Frozen)
        UNH$Frozen<-as.logical(UNH$Frozen) #convert to logical
  table(UNH$Parameter,useNA='ifany')
  table(UNH$Value,useNA='ifany')
  UNH$Units<-NA
  UNH$Rep<-NA
  table(UNH$Fluorometer,useNA='ifany')  #mostly missing
    UNH$Fluorometer<-ifelse(UNH$Fluorometer=='',NA,'Beagle')  #change 'Beagle Amniscience Fluorquick' to 'Beagle'
  # Add AnalysisYear  AnalysisMonth  AnalysisDay  Drop AnalysisDate
    table(UNH$AnalysisDate,useNA='ifany')
      UNH$AnalysisDate[UNH$AnalysisDate=='']<-'NA/NA/NA' #replace empty cells with NA/NA/NA
    a<-matrix(as.numeric(unlist(strsplit(UNH$AnalysisDate,'/'))),nrow=nrow(UNH),ncol=3,byrow=TRUE)
      UNH$AnalysisYear<-a[,3] 
      UNH$AnalysisMonth<-a[,1] 
      UNH$AnalysisDay<-a[,2]
      UNH<-UNH[,-which(names(UNH)=="AnalysisDate")]
  #parse AnalysisTime
    table(UNH$AnalysisTime,useNA='ifany') #All missing
      UNH$AnalysisHour<-NA
      UNH$AnalysisMinutes<-NA
        UNH<-UNH[,-which(names(UNH)=="AnalysisTime")]
  table(UNH$GPSType,useNA='ifany')
    UNH$GPSType[UNH$GPSType=='']<-NA #replace empty cells with NA
  #change field Photos from Y/N to TRUE/FALSE
    table(UNH$Photos,useNA='ifany')
      UNH$Photos[UNH$Photos=='']<-NA #replace empty cells with NA
      UNH$Photos<-gsub("Yes",TRUE,UNH$Photos)
      UNH$Photos<-gsub("No",FALSE,UNH$Photos)
        UNH$Photos<-as.logical(UNH$Photos) #convert to logical
  #change field Flag from Y/N to TRUE/FALSE
    table(UNH$Flag,useNA='ifany')
      UNH$Flag[UNH$Flag=='']<-NA #replace empty cells with NA
      UNH$Flag<-gsub("Yes",TRUE,UNH$Flag)
      UNH$Flag<-gsub("No",FALSE,UNH$Flag)
        UNH$Flag<-as.logical(UNH$Flag) #convert to logical
  as.data.frame(table(UNH$Comments,useNA='ifany'))
#reorder fields to match Data2014
  #consistency check
  ncol(UNH) #35
  ncol(Data2014) #35
  table(names(UNH)%in%names(Data2014))
  names(UNH)[27]
  #reorder
    UNH<-UNH[,names(Data2014)]
# add to data.frame Data2014
  Data2014<-rbind(Data2014,UNH)
```


### Maine

* Data in prep by Linda Bacon

### Rhode Island

* Data in prep by Linda Green & Elizabeth Herron

Save Data2014
-------------------------
After standardization the individual datasets have been combined into the data.frame "Data2014", this is saved as "Data2014.csv"

```{r Save, include=FALSE, echo=FALSE, cache=FALSE} 
  write.table(Data2014, file='Data2014.csv',row.names=F,sep=',') #write to csv
```







